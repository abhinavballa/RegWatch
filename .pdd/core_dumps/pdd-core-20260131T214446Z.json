{
  "schema_version": 1,
  "pdd_version": "0.0.135",
  "timestamp_utc": "20260131T214446Z",
  "argv": [
    "--context",
    "analysis_agent",
    "--force",
    "sync",
    "analysis_agent",
    "--agentic",
    "--budget",
    "20",
    "--max-attempts",
    "3"
  ],
  "cwd": "/tmp/pdd_connect_6082cf2d_s6ixj3dp",
  "platform": {
    "system": "Linux",
    "release": "6.9.12",
    "version": "#1 SMP Tue Nov 4 01:12:46 UTC 2025",
    "python": "3.12.3 (main, Jan  8 2026, 11:30:50) [GCC 13.3.0]"
  },
  "global_options": {
    "force": true,
    "strength": 1.0,
    "temperature": 0.0,
    "time": 0.25,
    "verbose": false,
    "quiet": false,
    "local": false,
    "context": "analysis_agent",
    "output_cost": null,
    "review_examples": false
  },
  "invoked_subcommands": [
    "sync"
  ],
  "total_cost": 0.0,
  "steps": [
    {
      "step": 1,
      "command": "sync",
      "cost": 0.0,
      "model": ""
    }
  ],
  "errors": [],
  "environment": {
    "PDD_BOT_USERNAME": "prompt-driven-github[bot]",
    "PDD_PATH": "/opt/pdd-repo/pdd",
    "PDD_OUTPUT_COST_PATH": "/tmp/pdd_costs.csv",
    "PATH": "/opt/venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin",
    "PDD_JWT_TOKEN": "<redacted>",
    "PDD_AUTO_UPDATE": "false",
    "PDD_ENV": "prod",
    "PDD_FORCE": "1",
    "PDD_SKIP_UPDATE_CHECK": "1"
  },
  "file_contents": {
    ".pdd/meta/voice_service_python.json": "{\n  \"pdd_version\": \"0.0.135\",\n  \"timestamp\": \"2026-01-31T20:48:46.758568+00:00\",\n  \"command\": \"test\",\n  \"prompt_hash\": \"731c866a592fee1ac9d0744a049e61b2cd2cc46f0b07e3b92d24edbe7b626b17\",\n  \"code_hash\": \"e7d47d1dac1d5171fff35c3b20eea2036de1b6c1d8ed4384d15c51e814c83c1a\",\n  \"example_hash\": \"471641a27734280bc8324d9a091641cde0d72c54bc0c5e6399dde7e5a1a113d6\",\n  \"test_hash\": \"8ff34a5dd55d8ad85b99c3b6c488db2f4d0aa57665f31e5373b09d070dcd2731\",\n  \"test_files\": {\n    \"test_voice_service.py\": \"8ff34a5dd55d8ad85b99c3b6c488db2f4d0aa57665f31e5373b09d070dcd2731\"\n  }\n}",
    ".pdd/meta/hipaa_encryption_checker_python.json": "{\n  \"pdd_version\": \"0.0.135\",\n  \"timestamp\": \"2026-01-31T21:10:33.114802+00:00\",\n  \"command\": \"test\",\n  \"prompt_hash\": \"adb9246d8e4ff2e3dcf7e2b23753e1020972669c8ef6421a4c87aad13ff1e3ab\",\n  \"code_hash\": \"a6497549b36fa2b589b5c544c44f9d70a99d185ecdb73e353ded721373ad0904\",\n  \"example_hash\": \"3b3bdcb42c5fa10b3d57191e440da4cf54e4292d1cecb4d8d2873438974ae686\",\n  \"test_hash\": \"1e882bdf14d0e4dd906d07f90e569f0d6c020cb037926df4c31fa960149def2b\",\n  \"test_files\": {\n    \"test_hipaa_encryption_checker.py\": \"1e882bdf14d0e4dd906d07f90e569f0d6c020cb037926df4c31fa960149def2b\"\n  }\n}",
    ".pdd/meta/hipaa_access_control_checker_python.json": "{\n  \"pdd_version\": \"0.0.135\",\n  \"timestamp\": \"2026-01-31T21:00:12.463269+00:00\",\n  \"command\": \"generate\",\n  \"prompt_hash\": null,\n  \"code_hash\": null,\n  \"example_hash\": null,\n  \"test_hash\": null,\n  \"test_files\": null\n}",
    ".pdd/meta/patient_data_validator_python.json": "{\n  \"pdd_version\": \"0.0.135\",\n  \"timestamp\": \"2026-01-31T21:27:33.716841+00:00\",\n  \"command\": \"test\",\n  \"prompt_hash\": \"cadeb11eb20bb80645a9bb41affa36fe109c8698743c34123527571176b8f054\",\n  \"code_hash\": \"f53458de373ea0954bfaf307b17b9d50da0651bb613f19f12f325d1da5b76864\",\n  \"example_hash\": \"af6c7d125b8bccdb513450fa684797bbcf8388dd4afd0db90bdb35974f9f7a93\",\n  \"test_hash\": \"d2dbf12aa871a3784a874817d4f4900d9effc5093515d80110d948a3d7ab5969\",\n  \"test_files\": {\n    \"test_patient_data_validator.py\": \"d2dbf12aa871a3784a874817d4f4900d9effc5093515d80110d948a3d7ab5969\"\n  }\n}",
    ".pdd/meta/impact_agent_python.json": "{\n  \"pdd_version\": \"0.0.135\",\n  \"timestamp\": \"2026-01-31T21:03:13.887807+00:00\",\n  \"command\": \"generate\",\n  \"prompt_hash\": null,\n  \"code_hash\": null,\n  \"example_hash\": null,\n  \"test_hash\": null,\n  \"test_files\": null\n}",
    ".pdd/meta/hipaa_audit_logging_checker_python.json": "{\n  \"pdd_version\": \"0.0.135\",\n  \"timestamp\": \"2026-01-31T21:20:27.648638+00:00\",\n  \"command\": \"test\",\n  \"prompt_hash\": \"92042745488ffcbf98a46dee680ef8ab85276a49e4db5a303dc3a4647e560f30\",\n  \"code_hash\": \"ca85c864eca731997086598ad5810dd4afae11ea15e461cd4aab3dd4be3a0e6c\",\n  \"example_hash\": \"72bd330a8060297a0e8fc19b4ece65b9d81d2da3874926bf991bc2bcc9da114e\",\n  \"test_hash\": \"f404b1a18bf8f2ff19ba9bcb5f8172a35c5b4b0ddc6bb7db746e2b9b4b632835\",\n  \"test_files\": {\n    \"test_hipaa_audit_logging_checker.py\": \"f404b1a18bf8f2ff19ba9bcb5f8172a35c5b4b0ddc6bb7db746e2b9b4b632835\"\n  }\n}",
    ".pdd/meta/change_tracker_python.json": "{\n  \"pdd_version\": \"0.0.135\",\n  \"timestamp\": \"2026-01-31T20:40:24.808295+00:00\",\n  \"command\": \"test\",\n  \"prompt_hash\": \"1f633862b104a9b6ababcd083ba4819feaaa2d1d6fede3814caab53d95629f1a\",\n  \"code_hash\": \"960a6d8be574824fa6920af9c4f76cbd45c829cc779571ef2a03e4aa934db5ac\",\n  \"example_hash\": \"b285e64cfa98a05898793ab6b217868bcb80dba97e9977a0a7b9bf194d0513ed\",\n  \"test_hash\": \"a7b9a8fc67163fd9dee1d83f22dd5dc05017572d1cc7f4f44f8d44e6ee087f82\",\n  \"test_files\": {\n    \"test_change_tracker.py\": \"a7b9a8fc67163fd9dee1d83f22dd5dc05017572d1cc7f4f44f8d44e6ee087f82\"\n  }\n}",
    ".pdd/meta/scraper_agent_python.json": "{\n  \"pdd_version\": \"0.0.135\",\n  \"timestamp\": \"2026-01-31T21:37:27.912320+00:00\",\n  \"command\": \"test\",\n  \"prompt_hash\": \"6ef65d060cbdd54b107f247390fc00a9840607b199507b07d8260331aee2a5ae\",\n  \"code_hash\": \"57aa80f806a7caa4239ed6e48b8a02841c33ea306339b3783e7599743f6ed719\",\n  \"example_hash\": \"4e229d2727a26f845de649862132b8fdef87da9c99cf48e7e37b060fead0391d\",\n  \"test_hash\": \"05f8d84227116b4112e1a9072d90439af1a495fdcfff7a65b20731ce715f5a07\",\n  \"test_files\": {\n    \"test_scraper_agent.py\": \"05f8d84227116b4112e1a9072d90439af1a495fdcfff7a65b20731ce715f5a07\"\n  }\n}"
  },
  "terminal_output": "=== STDOUT ===\n\u256d\u2500\u2500\u2500 PDD Sync Starting \u2500\u2500\u2500\u2500\u256e\n\u2502 Basename: analysis_agent \u2502\n\u2502 Languages: python        \u2502\n\u2502 Budget: $20.00           \u2502\n\u2502 Max Attempts: 3          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\ud83d\ude80 Syncing for language: python...\nRunning sync in headless mode (CI/non-TTY environment)...\nUsing .pddrc context: analysis_agent\nInput files:\n  prompt_file     \n/tmp/pdd_connect_6082cf2d_s6ixj3dp/prompts/analysis_agent_Python.prompt\nOutput files:\n  output          \n/tmp/pdd_connect_6082cf2d_s6ixj3dp/prompts/analysis_agent_Python_with_deps.promp\nt\nDetected language: python\nBasename: analysis_agent\nSuccessfully loaded prompt: insert_includes_LLM\nLoaded insert_includes_LLM prompt template\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Starting prompt preprocessing                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nProcessing XML include: ./context/insert/1/prompt_to_update.prompt\nWarning: File not found: context/insert/1/prompt_to_update.prompt\nProcessing XML include: ./context/insert/1/dependencies.prompt\nWarning: File not found: context/insert/1/dependencies.prompt\nProcessing XML include: ./context/insert/1/updated_prompt.prompt\nWarning: File not found: context/insert/1/updated_prompt.prompt\nProcessing XML include: ./context/insert/2/prompt_to_update.prompt\nWarning: File not found: context/insert/2/prompt_to_update.prompt\nProcessing XML include: ./context/insert/2/dependencies.prompt\nWarning: File not found: context/insert/2/dependencies.prompt\nProcessing XML include: ./context/insert/2/updated_prompt.prompt\nWarning: File not found: context/insert/2/updated_prompt.prompt\nDoubling curly brackets...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Preprocessing complete                                                       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nPreprocessed prompt template\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Step 1: Loading prompt templates                                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nSuccessfully loaded prompt: auto_include_LLM\nSuccessfully loaded prompt: extract_auto_include_LLM\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Step 2: Running summarize_directory                                          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nSuccessfully loaded prompt: summarize_file_LLM\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Step 3: Running auto_include_LLM prompt                                      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nUsing injected JWT token from PDD_JWT_TOKEN\nCloud execution failed (Authentication expired (Authentication failed: Token \nexpired, 1769895205 < 1769895882). Please re-authenticate with: pdd auth logout \n&& pdd auth login), falling back to local execution...\nSYNC kwargs[caching]: True; litellm.cache: <litellm.caching.caching.Cache object at 0x7fd5de9ba570>; kwargs.get('cache')['no-cache']: False\nFinal returned optional params: {'temperature': 1, 'thinking': {'type': 'enabled', 'budget_tokens': 32000}, 'max_tokens': 36096}\n_is_function_call: False\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nSYNC kwargs[caching]: True; litellm.cache: <litellm.caching.caching.Cache object at 0x7fd5de9ba570>; kwargs.get('cache')['no-cache']: False\nFinal returned optional params: {'temperature': 1, 'thinking': {'type': 'enabled', 'budget_tokens': 32000}, 'max_tokens': 36096}\n_is_function_call: False\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nSYNC kwargs[caching]: True; litellm.cache: <litellm.caching.caching.Cache object at 0x7fd5de9ba570>; kwargs.get('cache')['no-cache']: False\nFinal returned optional params: {'temperature': 1, 'thinking': {'type': 'enabled', 'budget_tokens': 32000}, 'max_tokens': 36096}\n_is_function_call: False\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nSYNC kwargs[caching]: True; litellm.cache: <litellm.caching.caching.Cache object at 0x7fd5de9ba570>; kwargs.get('cache')['no-cache']: False\nFinal returned optional params: {'temperature': 1, 'thinking': {'type': 'enabled', 'budget_tokens': 32000}, 'max_tokens': 36096}\n_is_function_call: False\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nSYNC kwargs[caching]: True; litellm.cache: <litellm.caching.caching.Cache object at 0x7fd5de9ba570>; kwargs.get('cache')['no-cache']: False\nFinal returned optional params: {'temperature': 1, 'thinking': {'type': 'enabled', 'budget_tokens': 32000}, 'max_tokens': 36096}\n_is_function_call: False\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nSYNC kwargs[caching]: True; litellm.cache: <litellm.caching.caching.Cache object at 0x7fd5de9ba570>; kwargs.get('cache')['no-cache']: False\nFinal returned optional params: {'temperature': 1, 'thinking': {'type': 'enabled', 'budget_tokens': 32000}, 'max_tokens': 36096}\n_is_function_call: False\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nSYNC kwargs[caching]: True; litellm.cache: <litellm.caching.caching.Cache object at 0x7fd5de9ba570>; kwargs.get('cache')['no-cache']: False\nFinal returned optional params: {'temperature': 1, 'thinking': {'type': 'enabled', 'budget_tokens': 32000}, 'max_tokens': 36096}\n_is_function_call: False\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nSYNC kwargs[caching]: True; litellm.cache: <litellm.caching.caching.Cache object at 0x7fd5de9ba570>; kwargs.get('cache')['no-cache']: False\nFinal returned optional params: {'temperature': 1, 'thinking': {'type': 'enabled', 'budget_tokens': 32000}, 'max_tokens': 36096}\n_is_function_call: False\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nSYNC kwargs[caching]: True; litellm.cache: <litellm.caching.caching.Cache object at 0x7fd5de9ba570>; kwargs.get('cache')['no-cache']: False\nFinal returned optional params: {'temperature': 1, 'thinking': {'type': 'enabled', 'budget_tokens': 32000}, 'max_tokens': 36096}\n_is_function_call: False\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nSYNC kwargs[caching]: True; litellm.cache: <litellm.caching.caching.Cache object at 0x7fd5de9ba570>; kwargs.get('cache')['no-cache']: False\nFinal returned optional params: {'temperature': 0.0, 'max_retries': 2, 'extra_body': {}}\nopenai.py: Received openai error - Connection error.\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nSYNC kwargs[caching]: True; litellm.cache: <litellm.caching.caching.Cache object at 0x7fd5de9ba570>; kwargs.get('cache')['no-cache']: False\nFinal returned optional params: {'temperature': 0.0, 'max_retries': 0, 'extra_body': {}}\nopenai.py: Received openai error - Connection error.\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nSYNC kwargs[caching]: True; litellm.cache: <litellm.caching.caching.Cache object at 0x7fd5de9ba570>; kwargs.get('cache')['no-cache']: False\nFinal returned optional params: {'temperature': 0.0, 'max_retries': 0, 'extra_body': {}}\nopenai.py: Received openai error - Connection error.\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nSYNC kwargs[caching]: True; litellm.cache: <litellm.caching.caching.Cache object at 0x7fd5de9ba570>; kwargs.get('cache')['no-cache']: False\nFinal returned optional params: {'temperature': 0.0, 'max_retries': 2, 'extra_body': {}}\nopenai.py: Received openai error - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nSYNC kwargs[caching]: True; litellm.cache: <litellm.caching.caching.Cache object at 0x7fd5de9ba570>; kwargs.get('cache')['no-cache']: False\nFinal returned optional params: {'temperature': 0.0, 'max_retries': 0, 'extra_body': {}}\nopenai.py: Received openai error - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nSYNC kwargs[caching]: True; litellm.cache: <litellm.caching.caching.Cache object at 0x7fd5de9ba570>; kwargs.get('cache')['no-cache']: False\nFinal returned optional params: {'temperature': 0.0, 'max_retries': 0, 'extra_body': {}}\nopenai.py: Received openai error - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nError in auto_include: All candidate models failed. Last error \n(AuthenticationError): litellm.AuthenticationError: AuthenticationError: \nOpenAIException - The api_key client option must be set either by passing \napi_key to the client or by setting the OPENAI_API_KEY environment variable\nError in insert_includes: All candidate models failed. Last error \n(AuthenticationError): litellm.AuthenticationError: AuthenticationError: \nOpenAIException - The api_key client option must be set either by passing \napi_key to the client or by setting the OPENAI_API_KEY environment variable\nError: All candidate models failed. Last error (AuthenticationError): \nlitellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key \nclient option must be set either by passing api_key to the client or by setting \nthe OPENAI_API_KEY environment variable\n                        PDD Sync Complete                         \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Language \u2503 Status \u2503 Cost (USD) \u2503 Details                       \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 python   \u2502 Failed \u2502    $0.0000 \u2502 Operation 'auto-deps' failed. \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Total time: 4.55s | Total cost: $0.0000 | Overall status: Failed \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n--- Command Execution Summary ---\n  Step 1 (sync): Cost: $0.000000, Model: \nTotal Estimated Cost: $0.000000\n-------------------------------------\n"
}