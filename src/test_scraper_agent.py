
import sys
from pathlib import Path

# Add project root to sys.path to ensure local code is prioritized
# This allows testing local changes without installing the package
project_root = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(project_root))

"""
Test Plan for scraper_agent.py

1. Z3 Formal Verification:
   - Verify the Deduplication Logic: Prove that the set of new regulations returned never contains IDs present in the history set.
   - Verify Exponential Backoff Sequence: Verify that the delay sequence generated by the backoff logic follows the geometric progression 2^(n-1).

2. Unit Tests:
   - `test_extract_regulation_parsing_success`: Verify that `extract_regulation` correctly parses HTML content to extract:
     - Regulation ID (using CFR pattern)
     - Publication Date (using standard format)
     - Title (from <title> tag)
     - Cleaned text (HTML tags removed)
   - `test_extract_regulation_fallback_strategies`: Verify fallback mechanisms:
     - ID extraction from Federal Register (FR) pattern.
     - ID extraction from URL path (fallback).
     - Date defaulting to current date if not found.
   - `test_extract_regulation_network_failure`: Verify that `extract_regulation` returns None when the network request fails (after retries).
   - `test_monitor_regulations_flow`: Verify the main monitoring loop:
     - Mocks `_perform_web_search` to return search results.
     - Mocks `extract_regulation` to return structured data.
     - Verifies that results are aggregated correctly.
   - `test_monitor_regulations_deduplication`: Verify that regulations present in `change_tracker` history are filtered out.
   - `test_monitor_regulations_missing_api_key`: Verify that the function handles missing `TOOLHOUSE_API_KEY` gracefully (logs error, returns empty list).
   - `test_rate_limiting_enforcement`: Verify that `_enforce_rate_limit` calls `time.sleep` when requests are too frequent.
   - `test_retry_logic_execution`: Verify that `_fetch_page_content` (decorated with retry) attempts the operation multiple times upon failure.
"""

import sys
import os
import time
import pytest
import logging
from unittest.mock import MagicMock, patch, ANY
from datetime import datetime, date
from z3 import *

# Add the directory containing the code under test to sys.path
# This path is derived from the prompt information
sys.path.append('/Users/trinav/personal/RegWatch/prompts')

import scraper_agent

# --- Z3 Formal Verification Tests ---

def test_z3_verify_deduplication_logic():
    """
    Formal verification of the deduplication set logic.
    We want to prove that for any Regulation R found, if R.id is in History H,
    then R is NOT in the Result set.
    """
    s = Solver()
    
    # Universe of Regulation IDs (integers for simplicity in Z3)
    RegID = IntSort()
    
    # Sets modeled as Arrays mapping ID to Boolean (True if in set)
    History = Array('History', RegID, BoolSort())
    Found = Array('Found', RegID, BoolSort())
    Result = Array('Result', RegID, BoolSort())
    
    # An arbitrary ID 'x'
    x = Int('x')
    
    # Constraint 1: Definition of Result set logic used in code
    # Result contains x IFF x is Found AND x is NOT in History
    s.add(ForAll([x], Result[x] == And(Found[x], Not(History[x]))))
    
    # We want to verify: It is impossible for an ID to be in Result AND in History
    # Negate the property we want to prove: Exists x such that (x in Result AND x in History)
    violation = Exists([x], And(Result[x], History[x]))
    
    s.add(violation)
    
    # If UNSAT, it means no such violation exists -> Logic is correct
    result = s.check()
    assert result == unsat, "Deduplication logic verification failed: Found a counter-example where a history item was included in results."

def test_z3_verify_backoff_sequence():
    """
    Formal verification of the exponential backoff sequence.
    The code uses: delay = 1, then delay *= 2.
    Sequence: 1, 2, 4...
    Formula: delay_n = 2^n (where n is retry attempt index 0, 1, 2)
    """
    s = Solver()
    
    # Function f(n) representing delay at retry n
    f = Function('f', IntSort(), IntSort())
    n = Int('n')
    
    # Recursive definition based on code:
    # delay = 1 (initial)
    # next_delay = delay * 2
    s.add(f(0) == 1)
    s.add(ForAll([n], Implies(n >= 0, f(n+1) == f(n) * 2)))
    
    # We want to verify that f(2) == 4 (Retry 1: 1s, Retry 2: 2s, Retry 3: 4s)
    # Wait, looking at code:
    # Attempt 1 failed -> sleep(1), delay becomes 2
    # Attempt 2 failed -> sleep(2), delay becomes 4
    # Attempt 3 failed -> sleep(4), delay becomes 8
    
    # Let's verify f(2) == 4
    s.add(f(2) != 4) # Negate the property
    
    result = s.check()
    assert result == unsat, "Backoff sequence verification failed: f(2) should be 4."

# --- Unit Tests ---

@pytest.fixture
def mock_env_setup():
    """Sets up environment variables for testing."""
    with patch.dict(os.environ, {"TOOLHOUSE_API_KEY": "test_key"}):
        yield

@pytest.fixture
def mock_change_tracker():
    """Mocks the change_tracker dependency."""
    with patch('scraper_agent.change_tracker') as mock_ct:
        mock_ct.get_change_history.return_value = {"changes": []}
        yield mock_ct

@pytest.fixture
def mock_toolhouse():
    """Mocks the Toolhouse SDK."""
    with patch('scraper_agent.Toolhouse') as mock_th:
        yield mock_th

class TestExtractRegulation:
    
    @patch('urllib.request.urlopen')
    def test_extract_regulation_parsing_success(self, mock_urlopen):
        """Test successful extraction of ID, Date, Title, and Text."""
        # Mock HTML content
        html_content = """
        <html>
            <head><title>HIPAA Security Rule</title></head>
            <body>
                <h1>Final Rule</h1>
                <p>Publication Date: January 15, 2023</p>
                <p>The text of 45 CFR 164.312 requires encryption.</p>
                <script>var x=1;</script>
            </body>
        </html>
        """
        mock_response = MagicMock()
        mock_response.read.return_value = html_content.encode('utf-8')
        mock_response.__enter__.return_value = mock_response
        mock_urlopen.return_value = mock_response

        result = scraper_agent.extract_regulation("http://hhs.gov/rule.html")

        assert result is not None
        assert result['regulation_id'] == "HIPAA-164.312"
        assert result['title'] == "HIPAA Security Rule"
        assert result['publication_date'] == "2023-01-15"
        assert "requires encryption" in result['full_text']
        assert "<script>" not in result['full_text']

    @patch('urllib.request.urlopen')
    def test_extract_regulation_fallback_strategies(self, mock_urlopen):
        """Test fallback ID extraction and date parsing."""
        # HTML with FR citation and ISO date
        html_content = """
        <html>
            <head><title>Generic Rule</title></head>
            <body>
                <p>Published: 2023-12-01</p>
                <p>Reference: 88 FR 12345</p>
            </body>
        </html>
        """
        mock_response = MagicMock()
        mock_response.read.return_value = html_content.encode('utf-8')
        mock_response.__enter__.return_value = mock_response
        mock_urlopen.return_value = mock_response

        result = scraper_agent.extract_regulation("http://federalregister.gov/doc.html")

        assert result is not None
        assert result['regulation_id'] == "FR-88-12345"
        assert result['publication_date'] == "2023-12-01"

    @patch('urllib.request.urlopen')
    def test_extract_regulation_url_fallback(self, mock_urlopen):
        """Test ID extraction from URL when text patterns fail."""
        html_content = "<html><title>No ID Here</title><body>Just text.</body></html>"
        mock_response = MagicMock()
        mock_response.read.return_value = html_content.encode('utf-8')
        mock_response.__enter__.return_value = mock_response
        mock_urlopen.return_value = mock_response

        url = "http://hhs.gov/policies/privacy-rule.html"
        result = scraper_agent.extract_regulation(url)

        assert result is not None
        assert result['regulation_id'] == "UNKNOWN-privacy-rule"
        # Date should default to today since none is in text
        assert result['publication_date'] == datetime.now().date().isoformat()

    @patch('urllib.request.urlopen')
    def test_extract_regulation_network_failure(self, mock_urlopen):
        """Test that None is returned on persistent network failure."""
        mock_urlopen.side_effect = Exception("Network Error")
        
        # We patch time.sleep to speed up the test
        with patch('time.sleep'):
            result = scraper_agent.extract_regulation("http://bad-url.com")
        
        assert result is None

class TestMonitorRegulations:

    @patch('scraper_agent._perform_web_search')
    @patch('scraper_agent.extract_regulation')
    def test_monitor_regulations_flow(self, mock_extract, mock_search, mock_env_setup, mock_change_tracker):
        """Test the main monitoring flow with new regulations found."""
        # Setup mocks
        mock_search.return_value = [{'url': 'http://hhs.gov/new-rule', 'title': 'New Rule'}]
        
        mock_extract.return_value = {
            "regulation_id": "HIPAA-164.500",
            "title": "New Rule",
            "publication_date": "2024-01-01",
            "full_text": "Content",
            "source_url": "http://hhs.gov/new-rule"
        }

        results = scraper_agent.monitor_regulations(["http://hhs.gov"])

        assert len(results) == 1
        assert results[0]['regulation_id'] == "HIPAA-164.500"
        mock_search.assert_called()
        mock_extract.assert_called_with('http://hhs.gov/new-rule')

    @patch('scraper_agent._perform_web_search')
    @patch('scraper_agent.extract_regulation')
    def test_monitor_regulations_deduplication(self, mock_extract, mock_search, mock_env_setup, mock_change_tracker):
        """Test that known regulations are filtered out."""
        # Setup history with an existing ID
        mock_change_tracker.get_change_history.return_value = {
            "changes": [{"regulation_id": "HIPAA-EXISTING"}]
        }

        # Search finds the same ID
        mock_search.return_value = [{'url': 'http://hhs.gov/existing'}]
        mock_extract.return_value = {
            "regulation_id": "HIPAA-EXISTING",
            "title": "Old Rule",
            "publication_date": "2020-01-01",
            "full_text": "Old Content",
            "source_url": "http://hhs.gov/existing"
        }

        results = scraper_agent.monitor_regulations(["http://hhs.gov"])

        assert len(results) == 0

    def test_monitor_regulations_missing_api_key(self):
        """Test behavior when API key is missing."""
        # Ensure env var is unset
        with patch.dict(os.environ, {}, clear=True):
            results = scraper_agent.monitor_regulations()
            assert results == []

class TestRateLimitingAndRetry:

    def test_rate_limiting_enforcement(self):
        """Test that _enforce_rate_limit sleeps if called too quickly."""
        domain = "http://test.com/page"
        
        with patch('time.time') as mock_time, patch('time.sleep') as mock_sleep:
            # First call: sets the timestamp
            mock_time.return_value = 100.0
            scraper_agent._enforce_rate_limit(domain)
            mock_sleep.assert_not_called()

            # Second call: 0.1s later (too fast, limit is 1.0s)
            mock_time.return_value = 100.1
            scraper_agent._enforce_rate_limit(domain)
            
            # Should sleep for approx 0.9s
            mock_sleep.assert_called()
            args, _ = mock_sleep.call_args
            assert 0.8 < args[0] < 1.0

    @patch('urllib.request.urlopen')
    def test_retry_logic_execution(self, mock_urlopen):
        """Test that _fetch_page_content retries 3 times on failure."""
        # Configure mock to fail 3 times then succeed
        mock_urlopen.side_effect = [
            Exception("Fail 1"),
            Exception("Fail 2"),
            Exception("Fail 3"),
            MagicMock(read=lambda: b"Success")
        ]

        with patch('time.sleep') as mock_sleep:
            # We call the private function directly to test the decorator
            # Note: _fetch_page_content is decorated in the module
            content = scraper_agent._fetch_page_content("http://retry.com")
        
        assert content == "Success"
        assert mock_urlopen.call_count == 4 # Initial + 3 retries
        assert mock_sleep.call_count == 3 # Sleep after each failure
        
        # Verify backoff times: 1, 2, 4
        sleep_calls = [args[0] for args, _ in mock_sleep.call_args_list]
        # Filter out rate limit sleeps if any (though rate limit is 0 in this mock context usually)
        # The decorator sleeps 1, 2, 4.
        # Note: _enforce_rate_limit also calls sleep. We need to distinguish.
        # However, since we mocked time.sleep globally, we capture all.
        # The retry logic sleeps are integers 1, 2, 4.
        assert 1 in sleep_calls
        assert 2 in sleep_calls
        assert 4 in sleep_calls